{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish a connection to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    dbname = \"dw_cs\", \n",
    "    user = \"postgres\", \n",
    "    host= 'localhost',\n",
    "    # host = '172.30.160.1',\n",
    "    password = \"postgres\",\n",
    "    port = 5432\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur.close()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check the indexes on a table\n",
    "\n",
    "def check_indexes(table_name):\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            indexname,\n",
    "            indexdef\n",
    "        FROM\n",
    "            pg_indexes\n",
    "        WHERE\n",
    "            tablename = '{table_name}';\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(query)      \n",
    "        \n",
    "        indexes = cur.fetchall()\n",
    "        \n",
    "        for index in indexes:\n",
    "            print(f\"Index Name: {index[0]}\")\n",
    "            print(f\"Index Definition: {index[1]}\\n\")\n",
    "\n",
    "\n",
    "# function to explain-analyze a query\n",
    "\n",
    "def explain_analyze(query, analyze = True):\n",
    "    conn.rollback()\n",
    "    with conn.cursor() as cur:\n",
    "        if analyze:\n",
    "            cur.execute(f\"EXPLAIN ANALYZE {query}\")\n",
    "        else:\n",
    "            cur.execute(f\"EXPLAIN {query}\")\n",
    "        explain = cur.fetchall()\n",
    "\n",
    "        for line in explain:\n",
    "            print(line[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step\n",
    "Compute size and time for executing the queries without additional structure support. Record the size of the result set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first value is the startup cost, the second the total cost.\n",
    "\n",
    "Startup Cost: This represents the amount of work the query planner estimates is required before the first row can be returned. For a sequential scan (Seq Scan), this value is typically very low or zero because the first row can be returned almost immediately.\n",
    "\n",
    "Total Cost: This represents the total estimated cost to execute the entire query. It is the sum of the startup cost and the cost to process all rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial database size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database size: 20532.98 MB\n",
      "Database size: 20.05 GB \n",
      "\n"
     ]
    }
   ],
   "source": [
    "conn.rollback()\n",
    "with conn.cursor() as cur:\n",
    "\n",
    "    cur.execute(\"SELECT pg_database_size('dw_cs');\")\n",
    "    all_rows = cur.fetchall()\n",
    "    print(f\"Database size: {all_rows[0][0] / (1024**2):.2f} MB\")\n",
    "    print(f\"Database size: {all_rows[0][0] / (1024**3):.2f} GB \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Name: nation_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX nation_pkey ON public.nation USING btree (n_nationkey)\n",
      "\n",
      "Index Name: part_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX part_pkey ON public.part USING btree (p_partkey)\n",
      "\n",
      "Index Name: supplier_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX supplier_pkey ON public.supplier USING btree (s_suppkey)\n",
      "\n",
      "Index Name: customer_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX customer_pkey ON public.customer USING btree (c_custkey)\n",
      "\n",
      "Index Name: idx_l_shipdate\n",
      "Index Definition: CREATE INDEX idx_l_shipdate ON public.lineitem USING btree (l_shipdate)\n",
      "\n",
      "Index Name: idx_l_partkey\n",
      "Index Definition: CREATE INDEX idx_l_partkey ON public.lineitem USING btree (l_partkey)\n",
      "\n",
      "Index Name: region_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX region_pkey ON public.region USING btree (r_regionkey)\n",
      "\n",
      "Index Name: partsupp_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX partsupp_pkey ON public.partsupp USING btree (ps_partkey, ps_suppkey)\n",
      "\n",
      "Index Name: orders_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX orders_pkey ON public.orders USING btree (o_orderkey)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = ['nation', 'part', 'supplier', 'customer', 'lineitem', 'region', 'partsupp', 'orders']\n",
    "\n",
    "conn.rollback()\n",
    "for table in a:\n",
    "    check_indexes(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.rollback()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SET enable_seqscan = on;\")\n",
    "    cur.execute(\"SET enable_indexscan = on;\")\n",
    "    cur.execute(\"SET enable_bitmapscan = off;\")\n",
    "    cur.execute(\"SET enable_indexonlyscan = off;\")\n",
    "    cur.execute(\"SET enable_tidscan = off;\")\n",
    "    cur.execute(\"SET enable_material = off;\")\n",
    "    cur.execute(\"SET enable_nestloop = on;\")\n",
    "    cur.execute(\"SET enable_mergejoin = on;\")\n",
    "    cur.execute(\"SET enable_hashjoin = off;\")\n",
    "    cur.execute(\"SET enable_hashagg = on;\")\n",
    "    cur.execute(\"SET enable_sort = on;\")\n",
    "    cur.execute(\"SET enable_partition_pruning = off;\")\n",
    "    cur.execute(\"SET enable_partitionwise_join = off;\")\n",
    "    cur.execute(\"SET enable_partitionwise_aggregate = off;\")\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"\"\"\n",
    "SELECT\n",
    "    l_returnflag,\n",
    "    l_linestatus,\n",
    "    SUM(l_quantity) AS sum_qty,\n",
    "    SUM(l_extendedprice) AS sum_base_price,\n",
    "    SUM(l_extendedprice * (1 - l_discount)) AS sum_disc_price,\n",
    "    SUM(l_extendedprice * (1 - l_discount) * (1 + l_tax)) AS sum_charge,\n",
    "    AVG(l_quantity) AS avg_qty,\n",
    "    AVG(l_extendedprice) AS avg_price,\n",
    "    AVG(l_discount) AS avg_disc,\n",
    "    COUNT(*) AS count_order\n",
    "FROM\n",
    "    lineitem\n",
    "WHERE\n",
    "    l_shipdate <= DATE '1998-12-01' - INTERVAL '90' DAY\n",
    "GROUP BY\n",
    "    l_returnflag,\n",
    "    l_linestatus\n",
    "ORDER BY\n",
    "    l_returnflag,\n",
    "    l_linestatus;\n",
    "\"\"\"\n",
    "\n",
    "query_10 = \"\"\"\n",
    "SELECT\n",
    "    c_custkey,\n",
    "    c_name,\n",
    "    SUM(l_extendedprice * (1 - l_discount)) AS revenue,\n",
    "    c_acctbal,\n",
    "    n_name,\n",
    "    c_address,\n",
    "    c_phone,\n",
    "    c_comment\n",
    "FROM\n",
    "    customer,\n",
    "    orders,\n",
    "    lineitem,\n",
    "    nation\n",
    "WHERE\n",
    "    c_custkey = o_custkey\n",
    "    AND l_orderkey = o_orderkey\n",
    "    AND o_orderdate >= DATE '1993-10-01'\n",
    "    AND o_orderdate < DATE '1993-10-01' + INTERVAL '3' MONTH\n",
    "    AND l_returnflag = 'R'\n",
    "    AND c_nationkey = n_nationkey\n",
    "GROUP BY\n",
    "    c_custkey,\n",
    "    c_name,\n",
    "    c_acctbal,\n",
    "    c_phone,\n",
    "    n_name,\n",
    "    c_address,\n",
    "    c_comment\n",
    "ORDER BY\n",
    "    revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "query_14 = \"\"\"\n",
    "SELECT\n",
    "    100.00 * SUM(CASE\n",
    "        WHEN p_type LIKE 'PROMO%'\n",
    "        THEN l_extendedprice * (1 - l_discount)\n",
    "        ELSE 0\n",
    "    END) / SUM(l_extendedprice * (1 - l_discount)) AS promo_revenue\n",
    "FROM\n",
    "    lineitem,\n",
    "    part\n",
    "WHERE\n",
    "    l_partkey = p_partkey\n",
    "    AND l_shipdate >= DATE '1995-09-01'\n",
    "    AND l_shipdate < DATE '1995-09-01' + INTERVAL '1' MONTH;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_17 = \"\"\"\n",
    "SELECT\n",
    "    SUM(l_extendedprice) / 7.0 AS avg_yearly\n",
    "FROM\n",
    "    lineitem,\n",
    "    part\n",
    "WHERE\n",
    "    l_partkey = p_partkey\n",
    "    AND p_brand = 'Brand#23'\n",
    "    AND p_container = 'MED BOX'\n",
    "    AND l_quantity < (\n",
    "        SELECT\n",
    "            0.2 * AVG(l_quantity)\n",
    "        FROM\n",
    "            lineitem\n",
    "        WHERE\n",
    "            p_partkey = l_partkey\n",
    "    );\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalize GroupAggregate  (cost=2300104.98..2300106.93 rows=6 width=236) (actual time=40026.145..40027.598 rows=4 loops=1)\n",
      "  Group Key: l_returnflag, l_linestatus\n",
      "  ->  Gather Merge  (cost=2300104.98..2300106.38 rows=12 width=236) (actual time=40026.121..40027.510 rows=12 loops=1)\n",
      "        Workers Planned: 2\n",
      "        Workers Launched: 2\n",
      "        ->  Sort  (cost=2299104.95..2299104.97 rows=6 width=236) (actual time=40019.085..40019.086 rows=4 loops=3)\n",
      "              Sort Key: l_returnflag, l_linestatus\n",
      "              Sort Method: quicksort  Memory: 27kB\n",
      "              Worker 0:  Sort Method: quicksort  Memory: 27kB\n",
      "              Worker 1:  Sort Method: quicksort  Memory: 27kB\n",
      "              ->  Partial HashAggregate  (cost=2299104.74..2299104.87 rows=6 width=236) (actual time=40019.046..40019.052 rows=4 loops=3)\n",
      "                    Group Key: l_returnflag, l_linestatus\n",
      "                    Batches: 1  Memory Usage: 24kB\n",
      "                    Worker 0:  Batches: 1  Memory Usage: 24kB\n",
      "                    Worker 1:  Batches: 1  Memory Usage: 24kB\n",
      "                    ->  Parallel Seq Scan on lineitem  (cost=0.00..1436970.35 rows=24632411 width=25) (actual time=0.571..24071.277 rows=19714203 loops=3)\n",
      "                          Filter: (l_shipdate <= '1998-09-02 00:00:00'::timestamp without time zone)\n",
      "                          Rows Removed by Filter: 281148\n",
      "Planning Time: 4.600 ms\n",
      "Execution Time: 40028.095 ms\n"
     ]
    }
   ],
   "source": [
    "explain_analyze(query_1, analyze = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort  (cost=3344325.21..3345720.08 rows=557947 width=279) (actual time=34178.035..34228.331 rows=381105 loops=1)\n",
      "  Sort Key: (sum((lineitem.l_extendedprice * ('1'::numeric - lineitem.l_discount)))) DESC\n",
      "  Sort Method: external merge  Disk: 71032kB\n",
      "  ->  GroupAggregate  (cost=2942891.02..3146127.82 rows=557947 width=279) (actual time=32019.398..33825.301 rows=381105 loops=1)\n",
      "        Group Key: customer.c_custkey, nation.n_name\n",
      "        ->  Incremental Sort  (cost=2942891.02..3132179.14 rows=557947 width=259) (actual time=32019.386..33270.901 rows=1147084 loops=1)\n",
      "              Sort Key: customer.c_custkey, nation.n_name\n",
      "              Presorted Key: customer.c_custkey\n",
      "              Full-sort Groups: 34124  Sort Method: quicksort  Average Memory: 31kB  Peak Memory: 31kB\n",
      "              ->  Nested Loop  (cost=2942890.72..3107071.53 rows=557947 width=259) (actual time=32019.299..32944.164 rows=1147084 loops=1)\n",
      "                    ->  Merge Join  (cost=2942890.57..3093377.71 rows=557947 width=159) (actual time=32018.946..32656.608 rows=1147084 loops=1)\n",
      "                          Merge Cond: (orders.o_custkey = customer.c_custkey)\n",
      "                          ->  Gather Merge  (cost=2942883.48..3007865.60 rows=557947 width=16) (actual time=32018.495..32163.144 rows=1147084 loops=1)\n",
      "                                Workers Planned: 2\n",
      "                                Workers Launched: 2\n",
      "                                ->  Sort  (cost=2941883.46..2942464.65 rows=232478 width=16) (actual time=31963.048..31991.561 rows=382361 loops=3)\n",
      "                                      Sort Key: orders.o_custkey\n",
      "                                      Sort Method: external merge  Disk: 10376kB\n",
      "                                      Worker 0:  Sort Method: external merge  Disk: 10344kB\n",
      "                                      Worker 1:  Sort Method: external merge  Disk: 10224kB\n",
      "                                      ->  Merge Join  (cost=2881503.92..2917185.84 rows=232478 width=16) (actual time=30892.596..31815.346 rows=382361 loops=3)\n",
      "                                            Merge Cond: (lineitem.l_orderkey = orders.o_orderkey)\n",
      "                                            ->  Sort  (cost=2333118.63..2348371.33 rows=6101082 width=16) (actual time=28008.588..28507.930 rows=4936061 loops=3)\n",
      "                                                  Sort Key: lineitem.l_orderkey\n",
      "                                                  Sort Method: external merge  Disk: 141048kB\n",
      "                                                  Worker 0:  Sort Method: external merge  Disk: 140712kB\n",
      "                                                  Worker 1:  Sort Method: external merge  Disk: 138816kB\n",
      "                                                  ->  Parallel Seq Scan on lineitem  (cost=0.00..1436970.35 rows=6101082 width=16) (actual time=0.235..26038.863 rows=4936061 loops=3)\n",
      "                                                        Filter: (l_returnflag = 'R'::bpchar)\n",
      "                                                        Rows Removed by Filter: 15059290\n",
      "                                            ->  Sort  (cost=548382.24..549811.16 rows=571566 width=8) (actual time=2883.933..2926.565 rows=573157 loops=3)\n",
      "                                                  Sort Key: orders.o_orderkey\n",
      "                                                  Sort Method: external merge  Disk: 10128kB\n",
      "                                                  Worker 0:  Sort Method: external merge  Disk: 10128kB\n",
      "                                                  Worker 1:  Sort Method: external merge  Disk: 10128kB\n",
      "                                                  ->  Seq Scan on orders  (cost=0.00..485912.00 rows=571566 width=8) (actual time=0.354..2630.513 rows=573157 loops=3)\n",
      "                                                        Filter: ((o_orderdate >= '1993-10-01'::date) AND (o_orderdate < '1994-01-01 00:00:00'::timestamp without time zone))\n",
      "                                                        Rows Removed by Filter: 14426843\n",
      "                          ->  Index Scan using customer_pkey on customer  (cost=0.43..74794.43 rows=1500000 width=147) (actual time=0.447..254.694 rows=1499998 loops=1)\n",
      "                    ->  Memoize  (cost=0.15..2.17 rows=1 width=108) (actual time=0.000..0.000 rows=1 loops=1147084)\n",
      "                          Cache Key: customer.c_nationkey\n",
      "                          Cache Mode: logical\n",
      "                          Hits: 1147059  Misses: 25  Evictions: 0  Overflows: 0  Memory Usage: 4kB\n",
      "                          ->  Index Scan using nation_pkey on nation  (cost=0.14..2.16 rows=1 width=108) (actual time=0.014..0.014 rows=1 loops=25)\n",
      "                                Index Cond: (n_nationkey = customer.c_nationkey)\n",
      "Planning Time: 5.300 ms\n",
      "Execution Time: 34270.155 ms\n"
     ]
    }
   ],
   "source": [
    "explain_analyze(query_10, analyze = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate  (cost=1800922.34..1800922.35 rows=1 width=32) (actual time=28850.050..28850.114 rows=1 loops=1)\n",
      "  ->  Merge Join  (cost=1676444.04..1786617.50 rows=817419 width=33) (actual time=27995.015..28642.754 rows=749223 loops=1)\n",
      "        Merge Cond: (part.p_partkey = lineitem.l_partkey)\n",
      "        ->  Index Scan using part_pkey on part  (cost=0.43..92913.43 rows=2000000 width=25) (actual time=0.233..356.607 rows=1999994 loops=1)\n",
      "        ->  Sort  (cost=1676443.20..1678486.75 rows=817419 width=16) (actual time=27994.771..28054.022 rows=749223 loops=1)\n",
      "              Sort Key: lineitem.l_partkey\n",
      "              Sort Method: external sort  Disk: 24288kB\n",
      "              ->  Gather  (cost=1000.00..1582197.72 rows=817419 width=16) (actual time=0.426..27496.978 rows=749223 loops=1)\n",
      "                    Workers Planned: 2\n",
      "                    Workers Launched: 2\n",
      "                    ->  Parallel Seq Scan on lineitem  (cost=0.00..1499455.82 rows=340591 width=16) (actual time=0.274..27658.105 rows=249741 loops=3)\n",
      "                          Filter: ((l_shipdate >= '1995-09-01'::date) AND (l_shipdate < '1995-10-01 00:00:00'::timestamp without time zone))\n",
      "                          Rows Removed by Filter: 19745610\n",
      "Planning Time: 10.148 ms\n",
      "Execution Time: 28856.126 ms\n"
     ]
    }
   ],
   "source": [
    "explain_analyze(query_14, analyze = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set hashjoin and hashaggregate off, we only analyze the situation with indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to put two indexes on l_returnflag and l_linestatus but we saw that they were not used. We have to note that they do not have many distinct values: (3,2) respectively, we could use a bitmap index, but in postgre it is not implemented.\n",
    "\n",
    "Let's see if with an index on shipdate we get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create index on lineitem: 42.97916102409363 seconds\n"
     ]
    }
   ],
   "source": [
    "conn.rollback()\n",
    "with conn.cursor() as cur:\n",
    "\n",
    "    start_time = time.time()\n",
    "    cur.execute(\"CREATE INDEX idx_l_shipdate ON lineitem (l_shipdate);\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Time to create index on lineitem: {end_time - start_time} seconds\")\n",
    "\n",
    "    cur.execute(\"SELECT pg_relation_size('idx_l_shipdate');\")\n",
    "    index_size = cur.fetchone()[0]\n",
    "    print(f\"Size of idx_l_shipdate ON lineitem (l_shipdate): {index_size/(1024**2)} MB\")\n",
    "    print(f\"Size of idx_l_shipdate ON lineitem (l_shipdate): {index_size/(1024**3)} GB\")\n",
    "\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an index also on return flag to see if it helps both in query 1 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.rollback()\n",
    "with conn.cursor() as cur:\n",
    "\n",
    "    # tried also hash index but it doesn't work, probably problem with connections\n",
    "    start_time = time.time()\n",
    "    cur.execute(\"CREATE INDEX idx_l_returnflag ON lineitem (l_returnflag);\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Time to create index on lineitem: {end_time - start_time} seconds\")\n",
    "\n",
    "    cur.execute(\"SELECT pg_relation_size('idx_l_returnflag');\")\n",
    "    index_size = cur.fetchone()[0]\n",
    "    print(f\"Size of idx_l_returnflag ON lineitem (l_returnflag): {index_size/(1024**2)} MB\")\n",
    "    print(f\"Size of idx_l_returnflag ON lineitem (l_returnflag): {index_size/(1024**3)} GB\")\n",
    "\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalize GroupAggregate  (cost=2958736.61..2958738.57 rows=6 width=236) (actual time=28251.919..28253.960 rows=4 loops=1)\n",
      "  Group Key: l_returnflag, l_linestatus\n",
      "  ->  Gather Merge  (cost=2958736.61..2958738.01 rows=12 width=236) (actual time=28251.903..28253.927 rows=12 loops=1)\n",
      "        Workers Planned: 2\n",
      "        Workers Launched: 2\n",
      "        ->  Sort  (cost=2957736.59..2957736.60 rows=6 width=236) (actual time=28234.882..28234.883 rows=4 loops=3)\n",
      "              Sort Key: l_returnflag, l_linestatus\n",
      "              Sort Method: quicksort  Memory: 27kB\n",
      "              Worker 0:  Sort Method: quicksort  Memory: 27kB\n",
      "              Worker 1:  Sort Method: quicksort  Memory: 27kB\n",
      "              ->  Partial HashAggregate  (cost=2957736.37..2957736.51 rows=6 width=236) (actual time=28234.829..28234.834 rows=4 loops=3)\n",
      "                    Group Key: l_returnflag, l_linestatus\n",
      "                    Batches: 1  Memory Usage: 24kB\n",
      "                    Worker 0:  Batches: 1  Memory Usage: 24kB\n",
      "                    Worker 1:  Batches: 1  Memory Usage: 24kB\n",
      "                    ->  Parallel Bitmap Heap Scan on lineitem  (cost=658763.41..2095601.99 rows=24632411 width=25) (actual time=1132.011..12862.081 rows=19714203 loops=3)\n",
      "                          Recheck Cond: (l_shipdate <= '1998-09-02 00:00:00'::timestamp without time zone)\n",
      "                          Rows Removed by Index Recheck: 270333\n",
      "                          Heap Blocks: exact=11794 lossy=368737\n",
      "                          ->  Bitmap Index Scan on idx_l_shipdate  (cost=0.00..643983.96 rows=59117786 width=0) (actual time=1133.983..1133.983 rows=59142609 loops=1)\n",
      "                                Index Cond: (l_shipdate <= '1998-09-02 00:00:00'::timestamp without time zone)\n",
      "Planning Time: 8.066 ms\n",
      "Execution Time: 28254.153 ms\n"
     ]
    }
   ],
   "source": [
    "conn.rollback()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SET enable_seqscan = off;\")\n",
    "    cur.execute(\"SET enable_indexscan = on;\")\n",
    "    cur.execute(\"SET enable_bitmapscan = on;\")\n",
    "    cur.execute(\"SET enable_indexonlyscan = on;\")\n",
    "    cur.execute(\"SET enable_tidscan = off;\")\n",
    "    cur.execute(\"SET enable_material = off;\")\n",
    "    cur.execute(\"SET enable_nestloop = off;\")\n",
    "    cur.execute(\"SET enable_mergejoin = off;\")\n",
    "    cur.execute(\"SET enable_hashjoin = on;\")\n",
    "    cur.execute(\"SET enable_sort = on;\")\n",
    "    conn.commit()\n",
    "\n",
    "explain_analyze(query_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an improvement using an index on l_shipdate with a bitmapscan. \n",
    "\n",
    "What I would suggest is to keep the index on l_shipdate since it may help us also in query 14, where shipdate has high selectivity and probably will help more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate and record the size of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: \n",
      " ('A', 'F', Decimal('377518399'), Decimal('566065727797.25'), Decimal('537759104278.0656'), Decimal('559276670892.116819'), Decimal('25.5009751030070973'), Decimal('38237.151008958546'), Decimal('0.05000657454024320463'), 14804077)\n",
      "Size of query_1 result table: 0.015625 MB\n"
     ]
    }
   ],
   "source": [
    "conn.rollback()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SET enable_seqscan = off;\")\n",
    "    cur.execute(\"SET enable_indexscan = on;\")\n",
    "    cur.execute(\"SET enable_bitmapscan = on;\")\n",
    "    cur.execute(\"SET enable_indexonlyscan = off;\")\n",
    "    cur.execute(\"SET enable_tidscan = off;\")\n",
    "    cur.execute(\"SET enable_material = off;\")\n",
    "    cur.execute(\"SET enable_nestloop = off;\")\n",
    "    cur.execute(\"SET enable_mergejoin = off;\")\n",
    "    cur.execute(\"SET enable_hashjoin = on;\")\n",
    "    cur.execute(\"SET enable_sort = on;\")\n",
    "    conn.commit()\n",
    "    \n",
    "with conn.cursor() as cur:\n",
    "    \n",
    "    cur.execute(f\"CREATE TEMP TABLE temp_result AS {query_1};\")\n",
    "    cur.execute(\"SELECT * FROM temp_result;\")\n",
    "    result = cur.fetchone()\n",
    "    print(f\"First row: \\n {result}\")\n",
    "    cur.execute(\"SELECT pg_total_relation_size('temp_result');\")\n",
    "    size = cur.fetchall()\n",
    "    print(f\"Size of query_1 result table: {size[0][0]/(1024**2)} MB\")\n",
    "    cur.execute(\"DROP TABLE temp_result;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order_Date is quite selective 3 months / 7*12 months, so probably an index can be beneficial.\n",
    "\n",
    "ordering by revenue, which is computed in the query, can't be optimised.\n",
    "\n",
    "what can be optimised is the join, where there are already indexes on the pks.\n",
    "\n",
    "l_return flag is not selective, so we don't put an index on it, we tried but it didn't help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create idx_o_orderdate ON orders (o_orderdate): 10.98435091972351 seconds\n"
     ]
    }
   ],
   "source": [
    "conn.rollback()\n",
    "with conn.cursor() as cur:\n",
    "\n",
    "    start_time = time.time()\n",
    "    cur.execute(\"CREATE INDEX idx_o_orderdate ON orders (o_orderdate);\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Time to create idx_o_orderdate ON orders (o_orderdate): {end_time - start_time} seconds\")\n",
    "\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort  (cost=34913159.69..34914554.56 rows=557947 width=279) (actual time=30022.294..30073.158 rows=381105 loops=1)\n",
      "  Sort Key: (sum((lineitem.l_extendedprice * ('1'::numeric - lineitem.l_discount)))) DESC\n",
      "  Sort Method: external merge  Disk: 71032kB\n",
      "  ->  Finalize GroupAggregate  (cost=34545629.72..34714962.30 rows=557947 width=279) (actual time=28501.064..29597.489 rows=381105 loops=1)\n",
      "        Group Key: customer.c_custkey, nation.n_name\n",
      "        ->  Gather Merge  (cost=34545629.72..34703338.40 rows=464956 width=279) (actual time=28501.055..29306.487 rows=450965 loops=1)\n",
      "              Workers Planned: 2\n",
      "              Workers Launched: 2\n",
      "              ->  Partial GroupAggregate  (cost=34544629.70..34648670.92 rows=232478 width=279) (actual time=28484.249..29265.211 rows=150322 loops=3)\n",
      "                    Group Key: customer.c_custkey, nation.n_name\n",
      "                    ->  Incremental Sort  (cost=34544629.70..34642858.97 rows=232478 width=259) (actual time=28484.228..29054.586 rows=382361 loops=3)\n",
      "                          Sort Key: customer.c_custkey, nation.n_name\n",
      "                          Presorted Key: customer.c_custkey\n",
      "                          Full-sort Groups: 11518  Sort Method: quicksort  Average Memory: 31kB  Peak Memory: 31kB\n",
      "                          Worker 0:  Full-sort Groups: 11560  Sort Method: quicksort  Average Memory: 32kB  Peak Memory: 32kB\n",
      "                          Worker 1:  Full-sort Groups: 11460  Sort Method: quicksort  Average Memory: 32kB  Peak Memory: 32kB\n",
      "                          ->  Nested Loop  (cost=34544629.31..34632397.46 rows=232478 width=259) (actual time=28483.664..28941.968 rows=382361 loops=3)\n",
      "                                ->  Merge Join  (cost=34544629.16..34626660.10 rows=232478 width=159) (actual time=28483.301..28840.073 rows=382361 loops=3)\n",
      "                                      Merge Cond: (orders.o_custkey = customer.c_custkey)\n",
      "                                      ->  Sort  (cost=34544628.62..34545209.81 rows=232478 width=16) (actual time=28482.715..28509.757 rows=382361 loops=3)\n",
      "                                            Sort Key: orders.o_custkey\n",
      "                                            Sort Method: external merge  Disk: 10320kB\n",
      "                                            Worker 0:  Sort Method: external merge  Disk: 10352kB\n",
      "                                            Worker 1:  Sort Method: external merge  Disk: 10272kB\n",
      "                                            ->  Merge Join  (cost=33760239.00..34519931.00 rows=232478 width=16) (actual time=24548.282..28206.482 rows=382361 loops=3)\n",
      "                                                  Merge Cond: (lineitem.l_orderkey = orders.o_orderkey)\n",
      "                                                  ->  Sort  (cost=33760235.51..33775488.22 rows=6101082 width=16) (actual time=24545.760..25065.088 rows=4936061 loops=3)\n",
      "                                                        Sort Key: lineitem.l_orderkey\n",
      "                                                        Sort Method: external merge  Disk: 140832kB\n",
      "                                                        Worker 0:  Sort Method: external merge  Disk: 140512kB\n",
      "                                                        Worker 1:  Sort Method: external merge  Disk: 139216kB\n",
      "                                                        ->  Parallel Index Scan using idx_l_returnflag on lineitem  (cost=0.56..32864087.24 rows=6101082 width=16) (actual time=0.755..22501.460 rows=4936061 loops=3)\n",
      "                                                              Index Cond: (l_returnflag = 'R'::bpchar)\n",
      "                                                  ->  Index Scan using orders_pkey on orders  (cost=0.43..725439.44 rows=571566 width=8) (actual time=0.477..2761.987 rows=573130 loops=3)\n",
      "                                                        Filter: ((o_orderdate >= '1993-10-01'::date) AND (o_orderdate < '1994-01-01 00:00:00'::timestamp without time zone))\n",
      "                                                        Rows Removed by Filter: 14426125\n",
      "                                      ->  Index Scan using customer_pkey on customer  (cost=0.43..74794.43 rows=1500000 width=147) (actual time=0.572..195.465 rows=1499990 loops=3)\n",
      "                                ->  Memoize  (cost=0.15..2.17 rows=1 width=108) (actual time=0.000..0.000 rows=1 loops=1147084)\n",
      "                                      Cache Key: customer.c_nationkey\n",
      "                                      Cache Mode: logical\n",
      "                                      Hits: 382499  Misses: 25  Evictions: 0  Overflows: 0  Memory Usage: 4kB\n",
      "                                      Worker 0:  Hits: 383820  Misses: 25  Evictions: 0  Overflows: 0  Memory Usage: 4kB\n",
      "                                      Worker 1:  Hits: 380690  Misses: 25  Evictions: 0  Overflows: 0  Memory Usage: 4kB\n",
      "                                      ->  Index Scan using nation_pkey on nation  (cost=0.14..2.16 rows=1 width=108) (actual time=0.015..0.015 rows=1 loops=75)\n",
      "                                            Index Cond: (n_nationkey = customer.c_nationkey)\n",
      "Planning Time: 6.181 ms\n",
      "Execution Time: 30106.214 ms\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SET enable_seqscan = off;\")\n",
    "    cur.execute(\"SET enable_indexscan = on;\")\n",
    "    cur.execute(\"SET enable_bitmapscan = off;\")\n",
    "    cur.execute(\"SET enable_indexonlyscan = on;\")\n",
    "    cur.execute(\"SET enable_tidscan = off;\")\n",
    "    cur.execute(\"SET enable_material = off;\")\n",
    "    cur.execute(\"SET enable_nestloop = on;\")\n",
    "    cur.execute(\"SET enable_mergejoin = on;\")\n",
    "    cur.execute(\"SET enable_hashjoin = off;\")\n",
    "    cur.execute(\"SET enable_sort = on;\")\n",
    "    conn.commit()\n",
    "\n",
    "explain_analyze(query_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we get a small improvement using the index on l_returnflag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate and record the size of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: \n",
      " (1237537, 'Customer#001237537', Decimal('884989.6657'), Decimal('7840.17'), 'RUSSIA                   ', 'FNG6WgB1mopyyY,ajQTU qUPW5o', '32-367-120-4327', 'nag carefully about the regular packages. carefully reg')\n",
      "Size of query_10 result table: 78.5078125 MB\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    \n",
    "    cur.execute(f\"CREATE TEMP TABLE temp_result AS {query_10};\")\n",
    "    cur.execute(\"SELECT * FROM temp_result;\")\n",
    "    result = cur.fetchone()\n",
    "    print(f\"First row: \\n {result}\")\n",
    "    cur.execute(\"SELECT pg_total_relation_size('temp_result');\")\n",
    "    size = cur.fetchall()\n",
    "    print(f\"Size of query_10 result table: {size[0][0]/(1024**2)} MB\")\n",
    "    cur.execute(\"DROP TABLE temp_result;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shipdate is selective and we know that we have already an index on it, so we can leverage it \n",
    "\n",
    "we know we have a btree index in p_partkey\n",
    "\n",
    "since l_partkey will be fundamental for the next query, we can create now the index and check if it helps/is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create idx_l_partkey ON lineitem (l_partkey): 48.23984098434448 seconds\n"
     ]
    }
   ],
   "source": [
    "conn.rollback()\n",
    "with conn.cursor() as cur:\n",
    "\n",
    "    start_time = time.time()\n",
    "    cur.execute(\"CREATE INDEX idx_l_partkey ON lineitem (l_partkey);\")    \n",
    "    end_time = time.time()\n",
    "    print(f\"Time to create idx_l_partkey ON lineitem (l_partkey): {end_time - start_time} seconds\")\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of idx_l_partkey: 429.5078125 MB\n",
      "Size of idx_l_partkey: 0.41944122314453125 GB\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "\n",
    "    cur.execute(\"SELECT pg_relation_size('idx_l_partkey');\")\n",
    "    index_size = cur.fetchone()[0]\n",
    "    print(f\"Size of idx_l_partkey: {index_size/(1024**2)} MB\")\n",
    "    print(f\"Size of idx_l_partkey: {index_size/(1024**3)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate  (cost=1752182.24..1752182.26 rows=1 width=32) (actual time=21935.922..21936.312 rows=1 loops=1)\n",
      "  ->  Merge Join  (cost=1627703.30..1737877.40 rows=817419 width=33) (actual time=21114.887..21729.452 rows=749223 loops=1)\n",
      "        Merge Cond: (part.p_partkey = lineitem.l_partkey)\n",
      "        ->  Index Scan using part_pkey on part  (cost=0.43..92913.43 rows=2000000 width=25) (actual time=0.314..320.667 rows=1999994 loops=1)\n",
      "        ->  Sort  (cost=1627702.87..1629746.42 rows=817419 width=16) (actual time=21114.556..21176.988 rows=749223 loops=1)\n",
      "              Sort Key: lineitem.l_partkey\n",
      "              Sort Method: external sort  Disk: 24288kB\n",
      "              ->  Gather  (cost=12155.11..1533457.40 rows=817419 width=16) (actual time=82.282..20044.966 rows=749223 loops=1)\n",
      "                    Workers Planned: 2\n",
      "                    Workers Launched: 2\n",
      "                    ->  Parallel Bitmap Heap Scan on lineitem  (cost=11155.11..1450715.50 rows=340591 width=16) (actual time=75.792..20475.960 rows=249741 loops=3)\n",
      "                          Recheck Cond: ((l_shipdate >= '1995-09-01'::date) AND (l_shipdate < '1995-10-01 00:00:00'::timestamp without time zone))\n",
      "                          Rows Removed by Index Recheck: 6279649\n",
      "                          Heap Blocks: exact=14193 lossy=117411\n",
      "                          ->  Bitmap Index Scan on idx_l_shipdate  (cost=0.00..10950.76 rows=817419 width=0) (actual time=73.053..73.053 rows=749223 loops=1)\n",
      "                                Index Cond: ((l_shipdate >= '1995-09-01'::date) AND (l_shipdate < '1995-10-01 00:00:00'::timestamp without time zone))\n",
      "Planning Time: 4.825 ms\n",
      "Execution Time: 21942.151 ms\n"
     ]
    }
   ],
   "source": [
    "conn.rollback()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SET enable_seqscan = on;\")\n",
    "    cur.execute(\"SET enable_indexscan = on;\")\n",
    "    cur.execute(\"SET enable_bitmapscan = on;\")\n",
    "    cur.execute(\"SET enable_indexonlyscan = off;\")\n",
    "    cur.execute(\"SET enable_nestloop = on;\")\n",
    "    cur.execute(\"SET enable_mergejoin = on;\")\n",
    "    cur.execute(\"SET enable_hashjoin = off;\")\n",
    "    cur.execute(\"SET enable_sort = on;\")\n",
    "    cur.execute(\"SET enable_hashagg = on;\")\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "explain_analyze(query_14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the index on l_partkey is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate and record size result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: \n",
      " (Decimal('16.6475949416150953'),)\n",
      "Size of query_14 result table: 0.015625 MB\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    \n",
    "    cur.execute(f\"CREATE TEMP TABLE temp_result AS {query_14};\")\n",
    "    cur.execute(\"SELECT * FROM temp_result;\")\n",
    "    result = cur.fetchone()\n",
    "    print(f\"First row: \\n {result}\")\n",
    "    cur.execute(\"SELECT pg_total_relation_size('temp_result');\")\n",
    "    size = cur.fetchall()\n",
    "    print(f\"Size of query_14 result table: {size[0][0]/(1024**2)} MB\")\n",
    "    cur.execute(\"DROP TABLE temp_result;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query takes a lot of time if we don't use indexes.\n",
    "\n",
    "The index on l_partkey is fundamental.\n",
    "\n",
    "We tried hash indexes on p_brand and p_container but they are not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Name: idx_l_shipdate\n",
      "Index Definition: CREATE INDEX idx_l_shipdate ON public.lineitem USING btree (l_shipdate)\n",
      "\n",
      "Index Name: idx_l_partkey\n",
      "Index Definition: CREATE INDEX idx_l_partkey ON public.lineitem USING btree (l_partkey)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_indexes('lineitem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate  (cost=9428617.44..9428617.45 rows=1 width=32) (actual time=10194.594..10194.595 rows=1 loops=1)\n",
      "  ->  Nested Loop  (cost=0.87..9428567.40 rows=20015 width=8) (actual time=5.428..10193.586 rows=5526 loops=1)\n",
      "        ->  Index Scan using part_pkey on part  (cost=0.43..102913.43 rows=2002 width=4) (actual time=0.422..397.491 rows=2044 loops=1)\n",
      "              Filter: ((p_brand = 'Brand#23'::bpchar) AND (p_container = 'MED BOX'::bpchar))\n",
      "              Rows Removed by Filter: 1997956\n",
      "        ->  Index Scan using idx_l_partkey on lineitem  (cost=0.44..4658.06 rows=11 width=17) (actual time=4.581..4.792 rows=3 loops=2044)\n",
      "              Index Cond: (l_partkey = part.p_partkey)\n",
      "              Filter: (l_quantity < (SubPlan 1))\n",
      "              Rows Removed by Filter: 27\n",
      "              SubPlan 1\n",
      "                ->  Aggregate  (cost=137.10..137.11 rows=1 width=32) (actual time=0.148..0.148 rows=1 loops=61385)\n",
      "                      ->  Index Scan using idx_l_partkey on lineitem lineitem_1  (cost=0.44..137.02 rows=33 width=5) (actual time=0.001..0.145 rows=31 loops=61385)\n",
      "                            Index Cond: (l_partkey = part.p_partkey)\n",
      "Planning Time: 0.738 ms\n",
      "Execution Time: 10194.622 ms\n"
     ]
    }
   ],
   "source": [
    "conn.rollback()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SET enable_seqscan = off;\")\n",
    "    cur.execute(\"SET enable_indexscan = on;\")\n",
    "    cur.execute(\"SET enable_bitmapscan = on;\")\n",
    "    cur.execute(\"SET enable_indexonlyscan = on;\")\n",
    "    cur.execute(\"SET enable_nestloop = on;\")\n",
    "    cur.execute(\"SET enable_mergejoin = on;\")\n",
    "    cur.execute(\"SET enable_hashjoin = off;\")\n",
    "    cur.execute(\"SET enable_sort = on;\")\n",
    "    cur.execute(\"SET enable_hashagg = on;\")\n",
    "    conn.commit()\n",
    "\n",
    "explain_analyze(query_17, analyze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Decimal('3295493.512857142857'),)\n",
      "Size of query_17 result table: 0.015625 MB\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    \n",
    "    cur.execute(f\"CREATE TEMP TABLE temp_result AS {query_17};\")\n",
    "    cur.execute(\"SELECT * FROM temp_result;\")\n",
    "    result = cur.fetchall()\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "    cur.execute(\"SELECT pg_total_relation_size('temp_result');\")\n",
    "    size = cur.fetchall()\n",
    "    print(f\"Size of query_17 result table: {size[0][0]/(1024**2)} MB\")\n",
    "    cur.execute(\"DROP TABLE temp_result;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Name: nation_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX nation_pkey ON public.nation USING btree (n_nationkey)\n",
      "\n",
      "Index Name: part_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX part_pkey ON public.part USING btree (p_partkey)\n",
      "\n",
      "Index Name: supplier_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX supplier_pkey ON public.supplier USING btree (s_suppkey)\n",
      "\n",
      "Index Name: customer_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX customer_pkey ON public.customer USING btree (c_custkey)\n",
      "\n",
      "Index Name: idx_l_shipdate\n",
      "Index Definition: CREATE INDEX idx_l_shipdate ON public.lineitem USING btree (l_shipdate)\n",
      "\n",
      "Index Name: idx_l_partkey\n",
      "Index Definition: CREATE INDEX idx_l_partkey ON public.lineitem USING btree (l_partkey)\n",
      "\n",
      "Index Name: region_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX region_pkey ON public.region USING btree (r_regionkey)\n",
      "\n",
      "Index Name: partsupp_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX partsupp_pkey ON public.partsupp USING btree (ps_partkey, ps_suppkey)\n",
      "\n",
      "Index Name: orders_pkey\n",
      "Index Definition: CREATE UNIQUE INDEX orders_pkey ON public.orders USING btree (o_orderkey)\n",
      "\n",
      "Index Name: idx_o_orderdate\n",
      "Index Definition: CREATE INDEX idx_o_orderdate ON public.orders USING btree (o_orderdate)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = ['nation', 'part', 'supplier', 'customer', 'lineitem', 'region', 'partsupp', 'orders']\n",
    "\n",
    "conn.rollback()\n",
    "for table in a:\n",
    "    check_indexes(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final db size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database size: 19076.58 MB\n",
      "Database size: 18.63 GB \n",
      "\n"
     ]
    }
   ],
   "source": [
    "conn.rollback()\n",
    "with conn.cursor() as cur:\n",
    "\n",
    "    cur.execute(\"SELECT pg_database_size('dw_cs');\")\n",
    "    all_rows = cur.fetchall()\n",
    "    print(f\"Database size: {all_rows[0][0] / (1024**2):.2f} MB\")\n",
    "    print(f\"Database size: {all_rows[0][0] / (1024**3):.2f} GB \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
